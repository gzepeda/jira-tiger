{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfd78a2-9b14-4b8f-9dd9-eab8a4285ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, LLM, Process\n",
    "from src.agents.tools.data_extraction import JiraDataExtraction\n",
    "from src.agents.tools.data_processing import JiraDataProcessing\n",
    "from src.agents.tools.data_analysis import ListJiraReports, ReadJiraReport, SaveJiraData, JsonFileOperations\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from crewai.tools import tool\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082a929-3c58-4aaa-97e1-7f0e4a989c8b",
   "metadata": {},
   "source": [
    "### LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769641bb-271d-4db4-9303-6b2c8387902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting roxy variables\n",
    "os.environ[\"OPENAI_API_URL\"] = \"http://127.0.0.1:8899/v1\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8899/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"dummy-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c38640-c648-42b8-b6e8-ff7afc419acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = LLM(model = 'gpt-4o') #'o1-preview')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cc726-5127-4b84-9156-01a20dca8dbc",
   "metadata": {},
   "source": [
    "### Analytics Engineer agent\n",
    "\n",
    "Tasks: Data Ingestion and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da25c58-bd8e-4f0c-941f-511454334ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_engineer_agent = Agent(\n",
    "    role=\"Senior Jira Analytics Engineer\",\n",
    "    goal=\"\"\"\n",
    "    Transform and optimize Jira project data into structured, analysis-ready formats while ensuring data quality, \n",
    "    completeness, and compliance with best practices for project analyst consumption.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    You are a specialized Data Integration Engineer with over 10 years of experience in Jira data processing and analytics. \n",
    "    Your core expertise includes:\n",
    "    \n",
    "    Technical Skills:\n",
    "    - Advanced Jira API integration and data extraction\n",
    "    - Data cleaning, transformation, and validation\n",
    "    - CSV and Markdown report generation\n",
    "    - Data quality assurance and validation\n",
    "    \n",
    "    Domain Knowledge:\n",
    "    - Deep understanding of Jira data structures and relationships\n",
    "    - Expertise in agile project management metrics\n",
    "    - Strong background in data documentation and reporting\n",
    "    \n",
    "    Best Practices:\n",
    "    - Implements robust error handling and data validation\n",
    "    - Ensures data consistency and standardization\n",
    "    - Maintains clear documentation and audit trails\n",
    "    - Follows data privacy and security guidelines\n",
    "    \n",
    "    Your primary focus is on delivering high-quality, actionable data that enables project analysts \n",
    "    to make informed decisions and generate valuable insights from Jira project information.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False  # Added to ensure direct handling of sensitive data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f62b05d-20d8-45f2-aa5a-330062f0cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingestion_task = Task(\n",
    "    description=\"\"\"\n",
    "    Extract and process Jira project data to create a structured CSV file containing the board overview.\n",
    "\n",
    "    Key Objectives:\n",
    "    1. Connect to Jira and extract project board data\n",
    "    2. Generate a clean, well-structured CSV file\n",
    "    3. Ensure all relevant project information is captured\n",
    "\n",
    "    Required Action:\n",
    "    Use the ingest_board_overview() action from the JiraDataExtraction tool with these mandatory parameters:\n",
    "    - project_id: {project_id} (Jira project identifier)\n",
    "    - labels: {labels} (List of labels to filter issues)\n",
    "\n",
    "    Success Criteria:\n",
    "    - Successfully connected to Jira API\n",
    "    - Data extracted without errors\n",
    "    - CSV file created with proper formatting\n",
    "    - All specified labels included in the extraction\n",
    "    - Data properly sanitized and structured\n",
    "\n",
    "    Output Format:\n",
    "    The CSV file should contain:\n",
    "    - One row per issue/item\n",
    "    - Consistent date formats\n",
    "    - No missing or corrupted data\n",
    "    - UTF-8 encoding\n",
    "\n",
    "    \"\"\",\n",
    "    expected_output=\"Full path to the generated CSV file containing the Jira board overview data\",\n",
    "    agent=analytics_engineer_agent,\n",
    "    tools=[JiraDataExtraction()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9da6151-195d-4e9f-9997-5a8c74a7da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processing_task = Task(\n",
    "    description=\"\"\"\n",
    "    Convert the Jira CSV data into individual team-specific markdown reports for the project analyst's review.\n",
    "\n",
    "    Key Objectives:\n",
    "    1. Filter Jira updates between {start_date} and {end_date}\n",
    "    2. Generate separate markdown reports for each team\n",
    "    3. Ensure reports contain only relevant updates within the specified date range\n",
    "\n",
    "    Required Action:\n",
    "    Use the create_teams_markdowns() action from the JiraDataProcessing tool with these parameters:\n",
    "    - csv_file: Path to the previously generated CSV file\n",
    "    - start_date: {start_date} (format: YYYY-MM-DD)\n",
    "    - end_date: {end_date} (format: YYYY-MM-DD)\n",
    "\n",
    "    Success Criteria:\n",
    "    - Each team should have its own markdown report\n",
    "    - Reports should only include updates within the specified date range\n",
    "    - Reports should be properly formatted for analyst review\n",
    "    - All markdown files should be saved in an organized directory structure\n",
    "\n",
    "    Note: Ensure the CSV file exists and is accessible before processing.\n",
    "    \"\"\",\n",
    "    expected_output=\"Directory path containing the generated markdown reports\",\n",
    "    agent=analytics_engineer_agent,\n",
    "    tools=[JiraDataProcessing()],\n",
    "    context=[data_ingestion_task]  # This task will wait for data_ingestion_task to complete\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5add0-4138-4393-a74e-aff74abd37a3",
   "metadata": {},
   "source": [
    "### Project Analyst Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb6b822-b168-428d-bebf-150eb2d8aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_analyst = Agent(\n",
    "    role=\"Senior Project Performance Analyst\",\n",
    "    goal=\"\"\"\n",
    "    Deliver comprehensive, data-driven analysis of team performance through Jira metrics, providing actionable insights \n",
    "    and maintaining effective communication channels with development teams to drive continuous improvement.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    You are a seasoned Project Performance Analyst with 10+ years of experience in agile environments, specializing in \n",
    "    team productivity analysis and process optimization.\n",
    "\n",
    "    Technical Expertise:\n",
    "    - Advanced analysis of Jira metrics and team performance indicators\n",
    "    - Deep understanding of agile development workflows and metrics\n",
    "    - Expert in interpreting issue relationships and dependencies\n",
    "    - Proficient in identifying patterns and trends in project data\n",
    "\n",
    "    Analytical Skills:\n",
    "    - Strategic issue analysis and root cause identification\n",
    "    - Data-driven decision-making and recommendation formulation\n",
    "    - Sprint performance evaluation and optimization\n",
    "    - Risk assessment and mitigation strategy development\n",
    "\n",
    "    Communication Excellence:\n",
    "    - Diplomatic and effective feedback delivery\n",
    "    - Clear and concise reporting style\n",
    "    - Engaging and positive communication approach\n",
    "    - Ability to maintain professional relationships while ensuring accountability\n",
    "\n",
    "    Personal Attributes:\n",
    "    - Known for combining professionalism with approachability\n",
    "    - Maintains a positive, solution-focused mindset\n",
    "    - Demonstrates emotional intelligence in team interactions\n",
    "    - Balances humor with professionalism in communications\n",
    "    - Expert at delivering constructive feedback without creating tension\n",
    "\n",
    "    Best Practices:\n",
    "    - Ensures all analyses are backed by concrete data\n",
    "    - Maintains confidentiality and data security\n",
    "    - Provides context-aware recommendations\n",
    "    - Follows up systematically while maintaining positive team dynamics\n",
    "    - Creates actionable, specific, and measurable improvement plans\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    "    allow_code_execution=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e75c0e-19c0-415f-82ad-2bdef47ccd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "start_date = \"2025-02-03\"\n",
    "end_date = \"2025-02-08\"\n",
    "\n",
    "md_path = \"teams-markdown/\" + start_date + \"_to_\" + end_date\n",
    "json_path = \"teams-json/\" + start_date + \"_to_\" + end_date\n",
    "\n",
    "#def check_corresponding_files(md_path: str, json_path: str) -> str:\n",
    "\n",
    "    \n",
    "def check_corresponding_files(result: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Check if there are corresponding .json files for each .md file by comparing filenames without extensions.\n",
    "    Lists the markdown files that don't have corresponding json files, including their two parent folders.\n",
    "    \n",
    "    Args:\n",
    "        md_path (str): Path to the directory containing markdown files\n",
    "        json_path (str): Path to the directory containing json files\n",
    "    \n",
    "    Returns:\n",
    "        str: A message indicating either unprocessed markdown files or confirmation that all files are processed\n",
    "    \"\"\"\n",
    "    # Get all md files and convert to lowercase for case-insensitive comparison\n",
    "    md_files = {\n",
    "        f.stem.lower(): f\"{f.parent.parent.name}/{f.parent.name}/report_{f.stem}.md\" \n",
    "        for f in Path(md_path).glob(\"*.md\")\n",
    "    }\n",
    "    \n",
    "    # Get all json files and convert to lowercase for case-insensitive comparison\n",
    "    json_files = {f.stem.lower() for f in Path(json_path).glob(\"*.json\")}\n",
    "    \n",
    "    # Find unprocessed markdown files (preserving path structure)\n",
    "    unprocessed_files = [md_files[name] for name in md_files.keys() if name not in json_files]\n",
    "    \n",
    "    if unprocessed_files:\n",
    "        return (False, f\"ERROR: The following reports still need to be processed: {', '.join(unprocessed_files)}\")\n",
    "    else:\n",
    "        return (True, \"SUCCESS: All the json files were generated as expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc54e477-4275-4987-a73e-d90bc512940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fup_generation_task = Task(\n",
    "    description=\"\"\"\n",
    "    ## Task Overview\n",
    "    Analyze team Jira reports and generate structured follow-ups between {start_date} and {end_date}.\n",
    "\n",
    "    ## Phase 1: Report Collection and Tracking\n",
    "    1. Use list_reports() action from ListJiraReports tool to get all markdown files:\n",
    "       ```\n",
    "       team_reports = list_reports(\"path_to_markdown_files\")\n",
    "       ```\n",
    "    2. IMPORTANT: Create a processing tracking list:\n",
    "       * Store the total number of reports to process: total_reports = len(team_reports)\n",
    "       * Keep track of processed reports: processed_reports = 0\n",
    "       * You must process ALL reports before completing the task\n",
    "\n",
    "    ## Phase 2: Sequential Report Processing\n",
    "    CRITICAL: You must follow this sequence for EACH report in team_reports:\n",
    "    1. Get current report path: current_report = team_reports[processed_reports]\n",
    "    2. Read report using read_report() from ReadJiraReport tool:\n",
    "       ```\n",
    "       team_data = read_report(current_report)\n",
    "       ```\n",
    "    3. Process report completely before moving to the next one\n",
    "    4. After processing and saving, increment: processed_reports += 1\n",
    "    5. Verify: processed_reports matches the report being handled\n",
    "\n",
    "    ## Processing Checkpoints\n",
    "    After each team processing:\n",
    "    * Confirm JSON was saved successfully\n",
    "    * Report current progress: \"Processed <processed_reports> out of <total_reports> teams\"\n",
    "    * List the name of the team just processed\n",
    "    * DO NOT proceed to next team until current team is fully processed\n",
    "    \n",
    "    Process the report according to the following structure:\n",
    "    ```\n",
    "    ## Team name: <team>\n",
    "    ### Points of Contact\n",
    "    [Contact list]\n",
    "    ### Updated Issues\n",
    "    [Recent updates]\n",
    "    ### Not Updated Issues\n",
    "    [Pending updates]\n",
    "    ### This ends all the issues from the team <team> ###\n",
    "\n",
    "    ## Phase 3: Analysis Workflow\n",
    "    For each team report:\n",
    "\n",
    "    1. EXTRACT CORE DATA\n",
    "       * Team name (exact match)\n",
    "       * Points of Contact list\n",
    "       * Updated issues section\n",
    "       * Non-updated issues section\n",
    "       * Parent-child issue mappings\n",
    "\n",
    "    2. PERFORM ISSUE ANALYSIS\n",
    "       For each parent issue:\n",
    "       * Analyze issue description\n",
    "       * Review all comments\n",
    "       * Examine child issues\n",
    "       * Generate follow-up addressing:\n",
    "         * Current progress\n",
    "         * Blocking issues\n",
    "         * Required clarifications\n",
    "         * Specific action items\n",
    "         * Hugo Zanini is the current project manager of multi-repos. He looks into the cards weekly. If there are comments from him, probably, are things you need to follow up too.\n",
    "\n",
    "\n",
    "    3. GENERATE JSON OUTPUT\n",
    "       Create a structured dictionary:\n",
    "       * Team Information:\n",
    "         * name: Exact team name\n",
    "         * contacts: [@firstnamelastname format] #Example: John Doe -> @johndoe\n",
    "       \n",
    "       * Issue Arrays:\n",
    "         * updated_issues: Recent activity\n",
    "         * no_update_issues: Pending updates\n",
    "         \n",
    "       * Issue Details:\n",
    "         * id: \"DBPD-737\"\n",
    "         * title: Complete issue title\n",
    "         * url: \"https://nubank.atlassian.net/browse/DBPD-737\"\n",
    "         * workstream: Specific workstream\n",
    "         * fup: Contextual follow-up comment\n",
    "    \n",
    "\n",
    "     4. SAVE & VERIFY OUTPUT\n",
    "         Use the tool save_data, from the SaveJiraData tool to save the json data.\n",
    "         \n",
    "       * Execute save_data:\n",
    "         ```\n",
    "         save_data(\n",
    "             data_dict=team_data, #In the dictionary format\n",
    "             file_name= <team-name>.json\",\n",
    "             base_path = teams_json\n",
    "             folder= {start_date}_to_{end_date}\"\n",
    "         )\n",
    "         ```\n",
    "       * Verify:\n",
    "         * File exists in teams_json/{start_date}_to_{end_date}/<team_name>.json\n",
    "         * Filename matches team\n",
    "         * All fields present and valid\n",
    "\n",
    "     ## Quality Guidelines\n",
    "        1. Processing Guarantees:\n",
    "           * Process teams in the exact order they appear in team_reports\n",
    "           * Never skip any team in the sequence\n",
    "           * Maintain a clear count of processed teams\n",
    "           * Report progress after each team completion\n",
    "           * Verify each team's JSON exists before proceeding\n",
    "\n",
    "        2. Completion Requirements:\n",
    "            * Task is NOT complete until processed_reports equals total_reports\n",
    "            * Generate summary of all processed teams at the end\n",
    "            * Verify all team JSONs exist in the output directory\n",
    "            * Report any processing issues immediately\n",
    "    \n",
    "        3. Follow-up Quality:\n",
    "           * Write professional, friendly communications\n",
    "           * Use @firstnamelastname format for mentions. Example: John Doe -> @johndoe\n",
    "           * Provide specific, actionable feedback\n",
    "           * Reference relevant context and updates\n",
    "           * Keep messages short and focused\n",
    "           * Use appropriate corporate humor (relaxed but professional)\n",
    "           * Include emojis for better readability\n",
    "           * Bold critical information\n",
    "           * Use bullet points for organization\n",
    "           * Formatting Rules:\n",
    "                * Always use \"\\n\" for line breaks\n",
    "                * Always use \"•\" for bullet points\n",
    "                * Always bold titles with *\n",
    "                * Always include relevant emojis\n",
    "    \n",
    "        4. Output Requirements:\n",
    "           * Only proceed to the next team report once you have saved the json of the team being analyzed\n",
    "           * Ensure JSON structure matches template\n",
    "           * Validate all required fields present\n",
    "           * Verify contact format compliance; **always** refer to people in the format @firstnamelastname.\n",
    "\n",
    "    ## Progress Tracking Format\n",
    "        After each team processing, report the following in the beginning of the next action:\n",
    "        ```\n",
    "        Team Processing Status:\n",
    "        - Team Name: [team name]\n",
    "        - Progress: [X] of [Y] teams processed\n",
    "        - JSON Status: Saved successfully at [path]\n",
    "        - Next Team: [next team name or \"Task Complete\"]\n",
    "        \n",
    "    # CRITICAL: YOU MUST NOT COMPLETE THE TASK UNTIL ALL TEAMS ARE PROCESSED\n",
    "\n",
    "        \"\"\",\n",
    "\n",
    "    expected_output= \"\"\"\n",
    "            Save the team JSON on the following format:\n",
    "            {{\n",
    "                \"name\": \"Exact Team Name\",\n",
    "                \"contacts\": [\"@firstnamelastname\", \"@firstnamelastname\"], \n",
    "                \"updated_issues\": [\n",
    "                    {{\n",
    "                        \"id\": \"ISSUE-KEY\",\n",
    "                        \"title\": \"Exact Issue Title\",\n",
    "                        \"url\": \"https://nubank.atlassian.net/browse/ISSUE-KEY\",\n",
    "                        \"workstream\": \"Exact Workstream Name\",\n",
    "                        \"fup\": \"Context-specific follow-up with @firstnamelastname mentions when needed\"\n",
    "                    }}\n",
    "                ],\n",
    "                \"no_update_issues\": [\n",
    "                    {{\n",
    "                        \"id\": \"ISSUE-KEY\",\n",
    "                        \"title\": \"Exact Issue Title\",\n",
    "                        \"url\": \"https://nubank.atlassian.net/browse/ISSUE-KEY\",\n",
    "                        \"workstream\": \"Exact Workstream Name\",\n",
    "                        \"fup\": \"Context-specific follow-up with @firstnamelastname mentions when needed\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\"\"\",\n",
    "    agent=project_analyst,\n",
    "    tools = [ListJiraReports(), ReadJiraReport(), SaveJiraData()],\n",
    "    context = [data_processing_task],\n",
    "    guardrail = check_corresponding_files,\n",
    "    max_retries=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4f32d5e-6e9c-4450-8e03-76141332031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fups_consolidation_task = Task(\n",
    "    description=\"\"\"\n",
    "    ## Task Overview\n",
    "    Consolidate all team-specific JSON reports into a single comprehensive JSON file for the period {start_date} to {end_date}.\n",
    "\n",
    "    ## Objective\n",
    "    Create a unified report that combines all individual team JSONs while maintaining data integrity and structure.\n",
    "\n",
    "    ## Required Action\n",
    "    Use the generate_consolidated_report action from the SaveJiraData tool with these parameters:\n",
    "    ```\n",
    "    generate_consolidated_report(\n",
    "        base_path=teams_json,  # Directory containing individual team JSONs\n",
    "        folder=\"{start_date}_to_{end_date}\"  # Target period folder\n",
    "    ) teams_json/{start_date}_to_{end_date}/slack_message.\n",
    "    ```\n",
    "\n",
    "    ## Success Criteria\n",
    "    1. All individual team JSONs successfully merged\n",
    "    2. Consolidated file maintains original data structure\n",
    "    3. No data loss during consolidation\n",
    "    4. Proper file naming and location\n",
    "\n",
    "    ## Validation Steps\n",
    "    - Verify consolidated JSON exists\n",
    "    - Confirm all teams are included\n",
    "    - Check data integrity\n",
    "    - Validate JSON format\n",
    "\n",
    "    # These keywords must never be translated and transformed:\n",
    "        - Action:\n",
    "        - Thought:\n",
    "        - Action Input:\n",
    "        because they are part of the thinking process instead of the output. \n",
    "\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    Path to the consolidated JSON file.\n",
    "    \"\"\",\n",
    "    agent=project_analyst,\n",
    "    tools = [SaveJiraData()],\n",
    "    context = [fup_generation_task]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5586fdef-0d2c-4f60-a686-d84b28963b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_generation_task = Task(\n",
    "    description=\"\"\"\n",
    "   # Slack Message Generation Task\n",
    "    Generate a formatted slack message in the JSON format from the consolidated JSON generated in the previous task.\n",
    "\n",
    "    ## Initial Data Loading\n",
    "    CRITICAL - First Step:\n",
    "    1. Get the path of the consolidated JSON file generated by the previous task\n",
    "    2. Use this path to load the data using read_json action:\n",
    "       ```\n",
    "       consolidated_data = read_json(file_path=<path from previous task>)\n",
    "       print(\"Data loaded successfully from previous task's output\")\n",
    "       ```\n",
    "\n",
    "    ## Data Processing Sequence\n",
    "    1. Data Cleaning:\n",
    "       REQUIRED: Remove all teams without points of contact from consolidated_data\n",
    "       STORE: Keep the filtered teams data for later use\n",
    "\n",
    "    2. Workstream Analysis:\n",
    "       A. CONSOLIDATE DATA:\n",
    "          * Extract consolidated report with follow-ups\n",
    "          * Separate work items by workstream:\n",
    "            - Multi-repos Incremental\n",
    "            - Multi-repos to scale\n",
    "\n",
    "       B. ANALYZE KEY ELEMENTS:\n",
    "          * Recent achievements and progress\n",
    "          * Current challenges and blockers\n",
    "          * Cross-team dependencies\n",
    "          * Resource constraints\n",
    "          * Timeline concerns\n",
    "\n",
    "       C. CREATE WORKSTREAM SUMMARIES:\n",
    "          For each workstream, generate text covering:\n",
    "          * Key accomplishments\n",
    "          * Major challenges\n",
    "          * Action items and next steps\n",
    "\n",
    "       D. CREATE A WORK SUMMARY:\n",
    "          * Create a concise paragraph (max two sentences) summarizing the week's evolution.\n",
    "\n",
    "    ## Message Formatting Requirements\n",
    "    CRITICAL - ALL messages MUST follow these rules:\n",
    "    1. Structure Rules:\n",
    "       * Use \"+\" for string concatenation\n",
    "       * Use \"\\n\" for line breaks\n",
    "       * Use \"•\" for bullet points\n",
    "       * Bold titles with *\n",
    "       * Include relevant emojis\n",
    "\n",
    "    2. Content Guidelines:\n",
    "       * Keep messages concise but specific. Executives should be able to read the summaries and understand what is going on without opening the Jira tickets.\n",
    "       * Use professional yet relaxed tone\n",
    "       * Include appropriate corporate humor\n",
    "       * Bold critical information\n",
    "       * Organize with bullet points\n",
    "\n",
    "    ## JSON Generation\n",
    "    CRITICAL - Follow these steps exactly:\n",
    "    1. Create the base dictionary structure:\n",
    "    {{\n",
    "        \"start_date\": \"<YYYY-MM-DD in text format>\",\n",
    "        \"end_date\": \"<YYYY-MM-DD in text format>\",\n",
    "        \"work_evolution\": \"Brief overview of overall progress and challenges\",\n",
    "        \"workstreams_summary\": \n",
    "            \"*:signal_strength: Multi-Repos Incremental*\\n\\n\" +\n",
    "            \"• *Achievement 1*: Detail\\n\" +\n",
    "            \"• *Achievement 2*: Detail\\n\\n\" +\n",
    "            \"*🚧 Challenges*\\n\" +\n",
    "            \"• Challenge 1\\n\" +\n",
    "            \"• Challenge 2\\n\\n\" +\n",
    "            \"*➡️ Next Steps*\\n\" +\n",
    "            \"• Action 1\\n\" +\n",
    "            \"• Action 2\\n\\n\" +\n",
    "            \"*:rocket: Multi-Repos to Scale*\\n\\n\" +\n",
    "            [Same structure for second workstream],\n",
    "        \"teams\": consolidated_data['teams']\n",
    "\n",
    "    }}\n",
    "\n",
    "    2. REQUIRED - Verify JSON Structure:\n",
    "       * start_date and end_date are in correct format\n",
    "       * work_evolution contains your summary\n",
    "       * workstreams_summary follows the required format\n",
    "       * teams array contains ALL teams from the original consolidated_data\n",
    "\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    ## Save Output\n",
    "    REQUIRED SEQUENCE:\n",
    "    1. Generate output path:\n",
    "       ```\n",
    "       output_path = f\"teams_json/{start_date}_to_{end_date}/slack_message.json\"\n",
    "       ```\n",
    "\n",
    "    2. Save complete JSON using save_json action:\n",
    "       ```\n",
    "       save_json(\n",
    "           file_path=output_path,\n",
    "           data=final_json  # Contains ALL required data\n",
    "       )\n",
    "       ```\n",
    "\n",
    "    ## Verification Checklist\n",
    "    CRITICAL - Verify before completing:\n",
    "    1. [ ] JSON contains ALL original teams data\n",
    "    2. [ ] No data loss from original consolidated_data\n",
    "    3. [ ] Only summaries and dates were updated\n",
    "    4. [ ] Team structure remains unchanged\n",
    "    5. [ ] File saved successfully\n",
    "\n",
    "    Return format:\n",
    "    \"Slack message JSON saved successfully at: <full_path_to_json>\"\n",
    "    \n",
    "    \"\"\",\n",
    "    agent=project_analyst,\n",
    "    tools=[JsonFileOperations()],\n",
    "    context = [fups_consolidation_task]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7aa72a-185b-439d-9c9d-eded8323b0db",
   "metadata": {},
   "source": [
    "### Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d05ce1e-54db-43ed-90d8-57d2e06eea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating crew\n",
    "jira_crew = Crew(\n",
    "    agents=[analytics_engineer_agent, project_analyst],\n",
    "    tasks=[data_ingestion_task, data_processing_task, \n",
    "           fup_generation_task, \n",
    "           fups_consolidation_task,\n",
    "           #report_generation_task\n",
    "          ],\n",
    "    verbose=True,\n",
    "    planning=True,\n",
    "    process=Process.sequential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44674427-f73c-402c-8b73-3aa3df1b6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'project_id': \"14130\",\n",
    "    'labels': [\"roadmap-mr-program-2025\"],\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = jira_crew.kickoff(inputs=inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
