{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üêÖ [Jira Tiger](https://github.com/hugozanini/jira-tiger.git)**\n",
        "#### `T`ickets `I`nsights `G`eneration and `E`fficient `R`eporting\n",
        "\n",
        "<table align=\"left\"><td>\n",
        "  <a target=\"_blank\"  href=\"https://colab.research.google.com/drive/1klKQdA3u-rJPtrqB_Qjs39hj1sWc5VbB?usp=sharing\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
        "  </a>\n",
        "</td><td>\n",
        "  <a target=\"_blank\"  href=\"https://github.com/hugozanini/jira-tiger\">\n",
        "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "</td></table>"
      ],
      "metadata": {
        "id": "HpTZ4z7ljNpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Environment Setup"
      ],
      "metadata": {
        "id": "kTR6WTojk1f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cloning and repo and installing the required libraries.\n",
        "**Run the following two cells just once to setup the environment.**\n",
        "\n",
        "*After the execution it is needed to restart the sesion.*"
      ],
      "metadata": {
        "id": "ck2K3Rcqw5dm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH7VdqgAhNIn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hugozanini/jira-tiger.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd jira-tiger/\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "b90wBVHzjelD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import the required libraries"
      ],
      "metadata": {
        "id": "jMniA4kkw0lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, LLM, Process\n",
        "from src.agents.tools.data_extraction import JiraDataExtraction\n",
        "from src.agents.tools.data_processing import JiraDataProcessing\n",
        "from src.agents.tools.data_analysis import ListJiraReports, ReadJiraReport, SaveJiraData, JsonFileOperations\n",
        "from src.agents.tools.slack_message import SlackMessage\n",
        "import yaml\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from crewai.tools import tool\n",
        "from IPython.display import Markdown\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "uURIWMZqw0P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load secrets as environment variables"
      ],
      "metadata": {
        "id": "CWVDK6lpz6QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "os.environ[\"JIRA_API_TOKEN\"] = userdata.get('JIRA_API_TOKEN')\n",
        "os.environ[\"JIRA_URL\"] = userdata.get('JIRA_URL')\n",
        "os.environ[\"JIRA_USERNAME\"] = userdata.get('JIRA_USERNAME')\n",
        "os.environ[\"SLACK_BOT_TOKEN\"] = userdata.get('SLACK_BOT_TOKEN')"
      ],
      "metadata": {
        "id": "84WwSxvIz5Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ü§ñ Model selection\n",
        "*LLM to be used by the agents*"
      ],
      "metadata": {
        "id": "IO6Mx1eFxDMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model = 'gemini/gemini-2.0-flash-lite-preview-02-05')"
      ],
      "metadata": {
        "id": "bgraWRqjw9KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Project Setup\n",
        "\n",
        "*Project-specific configurations.*"
      ],
      "metadata": {
        "id": "9aktymZoxknS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define start and end dates to be analyzed\n",
        "start_date = \"2025-02-18\"\n",
        "end_date = \"2025-02-25\"\n",
        "\n",
        "# Construct path for markdown and json files using the date range\n",
        "md_path = \"teams-markdown/\" + start_date + \"_to_\" + end_date\n",
        "json_path = \"teams_json/\" + start_date + \"_to_\" + end_date\n",
        "\n",
        "# Channel ID where the slack message will be sent\n",
        "channel_id = \"C0882AYL4H4\""
      ],
      "metadata": {
        "id": "CkV5kmmhxQsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üö£‚Äç‚ôÇÔ∏è Crew: Data Extraction and consolidation"
      ],
      "metadata": {
        "id": "wOVPUWgOybgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üë®‚Äçüíª Analytics Engineer agent\n",
        "\n",
        "üìù **Tasks**: Data Ingestion and Data Processing"
      ],
      "metadata": {
        "id": "GL-W8SFfyesE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analytics_engineer_agent = Agent(\n",
        "    role=\"Senior Jira Analytics Engineer\",\n",
        "    goal=\"\"\"\n",
        "    Transform and optimize Jira project data into structured, analysis-ready formats while ensuring data quality,\n",
        "    completeness, and compliance with best practices for project analyst consumption.\n",
        "    \"\"\",\n",
        "    backstory=\"\"\"\n",
        "    You are a specialized Data Integration Engineer with over 10 years of experience in Jira data processing and analytics.\n",
        "    Your core expertise includes:\n",
        "\n",
        "    Technical Skills:\n",
        "    - Advanced Jira API integration and data extraction\n",
        "    - Data cleaning, transformation, and validation\n",
        "    - CSV and Markdown report generation\n",
        "    - Data quality assurance and validation\n",
        "\n",
        "    Domain Knowledge:\n",
        "    - Deep understanding of Jira data structures and relationships\n",
        "    - Expertise in agile project management metrics\n",
        "    - Strong background in data documentation and reporting\n",
        "\n",
        "    Best Practices:\n",
        "    - Implements robust error handling and data validation\n",
        "    - Ensures data consistency and standardization\n",
        "    - Maintains clear documentation and audit trails\n",
        "    - Follows data privacy and security guidelines\n",
        "\n",
        "    Your primary focus is on delivering high-quality, actionable data that enables project analysts\n",
        "    to make informed decisions and generate valuable insights from Jira project information.\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    llm=llm,\n",
        "    allow_delegation=False,  # Added to ensure direct handling of sensitive data\n",
        "    embedder={\n",
        "        \"provider\": \"google\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "            \"api_key\": userdata.get('GEMINI_API_KEY'),\n",
        "        }\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "mEv3EO97x72q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù Data Ingestion Task"
      ],
      "metadata": {
        "id": "6z3p0HaAy_nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ingestion_task = Task(\n",
        "    description=\"\"\"\n",
        "    Extract and process Jira project data to create a structured CSV file containing the board overview.\n",
        "\n",
        "    Key Objectives:\n",
        "    1. Connect to Jira and extract project board data\n",
        "    2. Generate a clean, well-structured CSV file\n",
        "    3. Ensure all relevant project information is captured\n",
        "\n",
        "    Required Action:\n",
        "    Use the ingest_board_overview() action from the JiraDataExtraction tool with these mandatory parameters:\n",
        "    - project_id: {project_id} (Jira project identifier)\n",
        "    - labels: {labels} (List of labels to filter issues)\n",
        "\n",
        "    Success Criteria:\n",
        "    - Successfully connected to Jira API\n",
        "    - Data extracted without errors\n",
        "    - CSV file created with proper formatting\n",
        "    - All specified labels included in the extraction\n",
        "    - Data properly sanitized and structured\n",
        "\n",
        "    Output Format:\n",
        "    The CSV file should contain:\n",
        "    - One row per issue/item\n",
        "    - Consistent date formats\n",
        "    - No missing or corrupted data\n",
        "    - UTF-8 encoding\n",
        "\n",
        "    \"\"\",\n",
        "    expected_output=\"Full path to the generated CSV file containing the Jira board overview data\",\n",
        "    agent=analytics_engineer_agent,\n",
        "    tools=[JiraDataExtraction()]\n",
        ")\n"
      ],
      "metadata": {
        "id": "GjqNy6wGyrgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù Data Processing Task"
      ],
      "metadata": {
        "id": "jhbs8tuL1omd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_processing_task = Task(\n",
        "    description=\"\"\"\n",
        "    Convert the Jira CSV data into individual team-specific markdown reports for the project analyst's review.\n",
        "\n",
        "    Key Objectives:\n",
        "    1. Filter Jira updates between {start_date} and {end_date}\n",
        "    2. Generate separate markdown reports for each team\n",
        "    3. Ensure reports contain only relevant updates within the specified date range\n",
        "\n",
        "    Required Action:\n",
        "    Use the create_teams_markdowns() action from the JiraDataProcessing tool with these parameters:\n",
        "    - csv_file: Path to the previously generated CSV file\n",
        "    - start_date: {start_date} (format: YYYY-MM-DD)\n",
        "    - end_date: {end_date} (format: YYYY-MM-DD)\n",
        "\n",
        "    Success Criteria:\n",
        "    - Each team should have its own markdown report\n",
        "    - Reports should only include updates within the specified date range\n",
        "    - Reports should be properly formatted for analyst review\n",
        "    - All markdown files should be saved in an organized directory structure\n",
        "\n",
        "    Note: Ensure the CSV file exists and is accessible before processing.\n",
        "    \"\"\",\n",
        "    expected_output=\"Directory path containing the generated markdown reports\",\n",
        "    agent=analytics_engineer_agent,\n",
        "    tools=[JiraDataProcessing()],\n",
        "    context=[data_ingestion_task]  # This task will wait for data_ingestion_task to complete\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "hy0Ov-nE1axn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üî® Creating and running crew"
      ],
      "metadata": {
        "id": "XiaaHxEB1uDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating crew\n",
        "jira_crew = Crew(\n",
        "    agents=[analytics_engineer_agent],\n",
        "    tasks=[data_ingestion_task, data_processing_task],\n",
        "    verbose=True,\n",
        "    memory=False,\n",
        "    process=Process.sequential,\n",
        "    embedder={\n",
        "        \"provider\": \"google\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "            \"api_key\": userdata.get('GEMINI_API_KEY'),\n",
        "        }\n",
        "    }\n",
        "  )"
      ],
      "metadata": {
        "id": "8BC8LtHV1p4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\n",
        "    'project_id': \"14130\",\n",
        "    'labels': [\"data-platform-refactoring\"],\n",
        "    'start_date': start_date,\n",
        "    'end_date': end_date\n",
        "}"
      ],
      "metadata": {
        "id": "hq5NXFPH1wSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = jira_crew.kickoff(inputs=inputs)"
      ],
      "metadata": {
        "id": "mJ8k8Ymp11wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîé Human in the loop\n",
        "Look into the Markdowns to check everything was saved properly"
      ],
      "metadata": {
        "id": "zIDzpBMY9wy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üö£‚Äç‚ôÇÔ∏è Crew : Project updates analysis"
      ],
      "metadata": {
        "id": "2d34ywV797AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üë®‚Äçüíª Project Analyst Agent\n",
        "\n",
        "üìù **Tasks**: follow-ups generation, follow-ups consolidation, and report summary"
      ],
      "metadata": {
        "id": "CLouMihu9_J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_analyst = Agent(\n",
        "    role=\"Senior Project Performance Analyst\",\n",
        "    goal=\"\"\"\n",
        "    Deliver comprehensive, data-driven analysis of team performance through Jira metrics, providing actionable insights\n",
        "    and maintaining effective communication channels with development teams to drive continuous improvement.\n",
        "    \"\"\",\n",
        "    backstory=\"\"\"\n",
        "    You are a seasoned Project Performance Analyst with 10+ years of experience in agile environments, specializing in\n",
        "    team productivity analysis and process optimization.\n",
        "\n",
        "    Technical Expertise:\n",
        "    - Advanced analysis of Jira metrics and team performance indicators\n",
        "    - Deep understanding of agile development workflows and metrics\n",
        "    - Expert in interpreting issue relationships and dependencies\n",
        "    - Proficient in identifying patterns and trends in project data\n",
        "\n",
        "    Analytical Skills:\n",
        "    - Strategic issue analysis and root cause identification\n",
        "    - Data-driven decision-making and recommendation formulation\n",
        "    - Sprint performance evaluation and optimization\n",
        "    - Risk assessment and mitigation strategy development\n",
        "\n",
        "    Communication Excellence:\n",
        "    - Diplomatic and effective feedback delivery\n",
        "    - Clear and concise reporting style\n",
        "    - Engaging and positive communication approach\n",
        "    - Ability to maintain professional relationships while ensuring accountability\n",
        "\n",
        "    Personal Attributes:\n",
        "    - Known for combining professionalism with approachability\n",
        "    - Maintains a positive, solution-focused mindset\n",
        "    - Demonstrates emotional intelligence in team interactions\n",
        "    - Balances humor with professionalism in communications\n",
        "    - Expert at delivering constructive feedback without creating tension\n",
        "\n",
        "    Best Practices:\n",
        "    - Ensures all analyses are backed by concrete data\n",
        "    - Maintains confidentiality and data security\n",
        "    - Provides context-aware recommendations\n",
        "    - Follows up systematically while maintaining positive team dynamics\n",
        "    - Creates actionable, specific, and measurable improvement plans\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=llm,\n",
        "    allow_code_execution=False,\n",
        "    embedder={\n",
        "        \"provider\": \"google\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "            \"api_key\": userdata.get('GEMINI_API_KEY'),\n",
        "        }\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "P5UrjNSC13Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù Follow up generation task"
      ],
      "metadata": {
        "id": "zTkpCdfg-KFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fup_generation_template = \"\"\"\n",
        "    ## Task Overview\n",
        "    Analyze the Jira issues report for the {team} and generate structured follow-ups between {start_date} and {end_date}.\n",
        "\n",
        "\n",
        "    ## Phase 1: Sequential Report Processing\n",
        "    1. Read report using read_report() from ReadJiraReport tool:\n",
        "       ```\n",
        "       team_data = read_report({team_report_file})\n",
        "\n",
        "\n",
        "    ## Phase 2: Process the report according to the following structure:\n",
        "    ```\n",
        "    ## Team name: <team>\n",
        "    ### Points of Contact\n",
        "    [Contact list]\n",
        "    ### Updated Issues\n",
        "    [Recent updates]\n",
        "    ### Not Updated Issues\n",
        "    [Pending updates]\n",
        "    ### This ends all the issues from the team <team> ###\n",
        "\n",
        "    ## Phase 3: Analysis Workflow\n",
        "    For each team report:\n",
        "\n",
        "    1. EXTRACT CORE DATA\n",
        "       * Team name (exact match)\n",
        "       * Points of Contact list\n",
        "       * Updated issues section\n",
        "       * Non-updated issues section\n",
        "       * Parent-child issue mappings\n",
        "\n",
        "    2. PERFORM ISSUE ANALYSIS\n",
        "       For each parent issue:\n",
        "       * Analyze issue description\n",
        "       * Review all comments\n",
        "       * Examine child issues\n",
        "       * Generate follow-up addressing:\n",
        "         * Current progress\n",
        "         * Blocking issues\n",
        "         * Required clarifications\n",
        "         * Specific action items\n",
        "\n",
        "    3. GENERATE JSON OUTPUT\n",
        "       Create a structured dictionary:\n",
        "       * Team Information:\n",
        "         * name: Exact team name\n",
        "         * contacts: [@firstnamelastname format] #Example: John Doe -> @johndoe\n",
        "\n",
        "       * Issue Arrays:\n",
        "         * updated_issues: Recent activity\n",
        "         * no_update_issues: Pending updates\n",
        "\n",
        "       * Issue Details:\n",
        "         * id: \"DBPD-737\"\n",
        "         * title: Complete issue title\n",
        "         * url: \"https://company.atlassian.net/browse/DBPD-737\"\n",
        "         * workstream: Specific workstream\n",
        "         * fup: Contextual follow-up comment\n",
        "\n",
        "\n",
        "     4. SAVE & VERIFY OUTPUT\n",
        "         Use the tool save_data, from the SaveJiraData tool to save the json data.\n",
        "\n",
        "       * Execute save_data:\n",
        "         ```\n",
        "         save_data(\n",
        "             data_dict=team_data, #In the dictionary format\n",
        "             file_name= {team}.json\",\n",
        "             base_path = teams_json\n",
        "             folder= {start_date}_to_{end_date}\"\n",
        "         )\n",
        "         ```\n",
        "       # Verify:\n",
        "         * To guarantee the report was saved properly, guaratnte you are seeing the message \"Data successfully saved to filepath.json\"\n",
        "\n",
        "     ## Quality Guidelines\n",
        "           * Write professional, friendly communications\n",
        "           * Use @firstnamelastname format for mentions. Example: John Doe -> @johndoe\n",
        "           * Provide specific, actionable feedback\n",
        "           * Reference relevant context and updates\n",
        "           * Keep messages short and focused\n",
        "           * Use appropriate corporate humor (relaxed but professional)\n",
        "           * Include emojis for better readability\n",
        "           * Bold critical information\n",
        "           * Use bullet points for organization\n",
        "           * Formatting Rules:\n",
        "                * Always use \"\\n\" for line breaks\n",
        "                * Always use \"‚Ä¢\" for bullet points\n",
        "                * Always bold titles with *\n",
        "                * Always include relevant emojis\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AbfuVoin-CiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fup_generation_output_template = \"\"\"\n",
        "Save the team JSON on the following format:\n",
        "{{\n",
        "    \"name\": \"Exact Team Name\",\n",
        "    \"contacts\": [\"@firstnamelastname\", \"@firstnamelastname\"],\n",
        "    \"updated_issues\": [\n",
        "        {{\n",
        "            \"id\": \"ISSUE-KEY\",\n",
        "            \"title\": \"Exact Issue Title\",\n",
        "            \"url\": \"https://company.atlassian.net/browse/ISSUE-KEY\",\n",
        "            \"workstream\": \"Exact Workstream Name\",\n",
        "            \"fup\": \"Context-specific follow-up with @firstnamelastname mentions when needed\"\n",
        "        }}\n",
        "    ],\n",
        "    \"no_update_issues\": [\n",
        "        {{\n",
        "            \"id\": \"ISSUE-KEY\",\n",
        "            \"title\": \"Exact Issue Title\",\n",
        "            \"url\": \"https://company.atlassian.net/browse/ISSUE-KEY\",\n",
        "            \"workstream\": \"Exact Workstream Name\",\n",
        "            \"fup\": \"Context-specific follow-up with @firstnamelastname mentions when needed\"\n",
        "        }}\n",
        "    ]\n",
        "}}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dD1mCVaR-V-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_team_reports_dict(md_path):\n",
        "    # Initialize empty dictionary\n",
        "    team_reports = {}\n",
        "\n",
        "    # List all files in the directory\n",
        "    for filename in os.listdir(md_path):\n",
        "        # Check if file is a markdown file\n",
        "        if filename.endswith('.md'):\n",
        "            # Extract team name by removing both '_report' and '.md' from filename\n",
        "            team_name = filename.replace('.md', '')\n",
        "            team_name = team_name.replace('._', ' ')\n",
        "            # Create full file path\n",
        "            file_path = os.path.join(md_path, filename)\n",
        "            # Add to dictionary\n",
        "            team_reports[team_name] = file_path\n",
        "\n",
        "    return team_reports"
      ],
      "metadata": {
        "id": "CTQzYDw2-Xi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_reports = get_team_reports_dict(md_path)"
      ],
      "metadata": {
        "id": "Amx2rM7b-ZpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fup_tasks = [\n",
        "    Task(\n",
        "        description=fup_generation_template.format(\n",
        "            team=team_name,\n",
        "            team_report_file=report_file,\n",
        "            base_path=json_path,\n",
        "            start_date = start_date,\n",
        "            end_date = end_date\n",
        "        ),\n",
        "        expected_output=fup_generation_output_template,\n",
        "        agent=project_analyst,\n",
        "        tools = [ReadJiraReport(), SaveJiraData()],\n",
        "    )\n",
        "    for team_name, report_file in team_reports.items()\n",
        "]"
      ],
      "metadata": {
        "id": "z6R4q3Xq-asM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù Follow up consolidation task"
      ],
      "metadata": {
        "id": "cCqwuT6a-n2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fups_consolidation_task = Task(\n",
        "    description=\"\"\"\n",
        "    ## Task Overview\n",
        "    Consolidate all team-specific JSON reports into a single comprehensive JSON file for the period {start_date} to {end_date}.\n",
        "\n",
        "    ## Objective\n",
        "    Create a unified report that combines all individual team JSONs while maintaining data integrity and structure.\n",
        "\n",
        "    ## Required Action\n",
        "    Use the generate_consolidated_report action from the SaveJiraData tool with these parameters:\n",
        "    ```\n",
        "    generate_consolidated_report(\n",
        "        base_path=teams_json,  # Directory containing individual team JSONs\n",
        "        folder=\"{start_date}_to_{end_date}\"  # Target period folder\n",
        "    ).\n",
        "    ```\n",
        "\n",
        "    ## Success Criteria\n",
        "    1. All individual team JSONs successfully merged\n",
        "    2. Consolidated file maintains original data structure\n",
        "    3. No data loss during consolidation\n",
        "    4. Proper file naming and location\n",
        "\n",
        "    ## Validation Steps\n",
        "    - Verify consolidated JSON exists\n",
        "    - Confirm all teams are included\n",
        "    - Check data integrity\n",
        "    - Validate JSON format\n",
        "\n",
        "    # These keywords must never be translated and transformed:\n",
        "        - Action:\n",
        "        - Thought:\n",
        "        - Action Input:\n",
        "        because they are part of the thinking process instead of the output.\n",
        "\n",
        "    \"\"\",\n",
        "    expected_output=\"\"\"\n",
        "    Path to the consolidated JSON file.\n",
        "    \"\"\",\n",
        "    agent=project_analyst,\n",
        "    tools = [SaveJiraData()],\n",
        ")"
      ],
      "metadata": {
        "id": "MvjociyW-i9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù Report summary task"
      ],
      "metadata": {
        "id": "0geXDmj7-xNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_summary_task = Task(\n",
        "    description=\"\"\"\n",
        "    # Slack Message Generation Task\n",
        "\n",
        "    ## Objective\n",
        "    Analyze a JSON file containing a consolidated report of team progress between {start_date} and {end_date}. Generate and save a structured output JSON summarizing key accomplishments, challenges, and next steps for each workstream.\n",
        "\n",
        "    ## Input\n",
        "    - A JSON file located at `{json_path}/{start_date}_to_{end_date}.json`.\n",
        "    - The input contains fields such as:\n",
        "      * `start_date`, `end_date`\n",
        "      * `work_evolution`\n",
        "      * `workstreams_summary`\n",
        "      * A list of `teams`, each with:\n",
        "        - `name`\n",
        "        - A list of `contacts`\n",
        "        - A list of `updated_issues` (if any)\n",
        "        - A list of `no_update_issues` (if any)\n",
        "\n",
        "    ## Process\n",
        "    1. Load the input JSON:\n",
        "       ```\n",
        "       consolidated_data = read_json(file_path=f\"{json_path}/{start_date}_to_{end_date}.json\")\n",
        "       print(\"Data loaded successfully.\")\n",
        "       ```\n",
        "       Handle errors gracefully if the file cannot be loaded.\n",
        "\n",
        "    2. Analyze key elements:\n",
        "       * Recent achievements and progress\n",
        "       * Current challenges and blockers\n",
        "       * Cross-team dependencies\n",
        "       * Resource constraints\n",
        "       * Timeline concerns\n",
        "\n",
        "    3. Generate summaries:\n",
        "       A. Workstream Summaries: For each workstream, include:\n",
        "          * Accomplishments\n",
        "          * Challenges\n",
        "          * Action items and next steps\n",
        "\n",
        "       B. Weekly Summary: Write a concise paragraph summarizing overall progress.\n",
        "\n",
        "    ## Output\n",
        "    Save the results as a structured JSON file at:\n",
        "    ```\n",
        "    output_path = f\"teams_json/{start_date}_to_{end_date}/report_summary.json\"\n",
        "    save_json(file_path=output_path, data=<dictionary(json) generated by you> )\n",
        "    ```\n",
        "\n",
        "    ## Content Guidelines\n",
        "    - Keep messages concise but specific.\n",
        "    - Use a professional yet approachable tone.\n",
        "    - Incorporate light corporate humor where appropriate.\n",
        "    - Highlight critical information using bold text\n",
        "    - Use emojis sparingly to enhance readability\n",
        "    * Formatting Rules:\n",
        "        * Always use \"\\n\" for line breaks\n",
        "        * Always use \"‚Ä¢\" for bullet points\n",
        "        * Always bold titles with *\n",
        "        * Guarantee the final json is properly formated, as indicated in the expected output, without unecessary \"\\n\" or spaces\n",
        "\n",
        "    # Only proceed after saving at teams_json/{start_date}_to_{end_date}/report_summary.json the dictionary generated by you.\n",
        "\n",
        "    \"\"\",\n",
        "    expected_output=\"\"\"\n",
        "        Example of expected output to be saved:\n",
        "        {{\n",
        "          \"start_date\": \"January 5, 2025\",\n",
        "          \"end_date\": \"January 12, 2025\",\n",
        "          \"work_evolution\": \"The Data Platform Refactoring program has seen steady progress across both infrastructure and governance workstreams. Focus has been placed on setting a solid foundation and addressing key challenges related to security, monitoring, and initial data quality frameworks. Continued attention will be required to drive adoption of new standards and ensure seamless integration of refactored components.\",\n",
        "          \"workstreams_summary\": \"*:wrench: Data Infrastructure*\\n\\n\" +\n",
        "            \"‚Ä¢ *Networking Foundation*: Initial network security policies configured, focusing on restricting access and implementing core firewall rules. VPN connection established.\\n\" +\n",
        "            \"‚Ä¢ *Telemetry & Monitoring*: Basic Cloud Logging sink setup initiated; working on defining standardized logging formats.\\n\" +\n",
        "            \"‚Ä¢ *Compute Optimization*: Exploration of Dataproc cluster configuration, including autoscaling, to optimize resource utilization.\\n\" +\n",
        "            \"‚Ä¢ *Service Account Security*: Initial steps taken to audit service account usage, identifying potential security risks related to external access.\\n\\n\" +\n",
        "            \"*üöß Challenges*\\n\" +\n",
        "            \"‚Ä¢ Defining clear standards to follow.\\n\" +\n",
        "            \"‚Ä¢ Need to make the infrastructure secure from all sides\\n\\n\" +\n",
        "            \"*‚û°Ô∏è Next Steps*\\n\" +\n",
        "            \"‚Ä¢ Finalize and document core networking architecture.\\n\" +\n",
        "            \"‚Ä¢ Fully implement Cloud Logging sink and alerts for critical infrastructure events.\\n\" +\n",
        "            \"‚Ä¢ Define clear guidelines on service accounts.\\n\\n\\n\" +\n",
        "            \"*:chart_with_upwards_trend: Data Gov and Experience*\\n\\n\" +\n",
        "            \"‚Ä¢ *Governance Framework*: Initial data governance policies and standards drafted, focusing on access requirements and tagging conventions.\\n\" +\n",
        "            \"‚Ä¢ *Data Quality*: Data reconciliation processes are being tested and data validation rules are being created.\\n\" +\n",
        "            \"‚Ä¢ *Documentation*: Training documentation is being created to allow data scientists to use the platform.\\n\" +\n",
        "            \"‚Ä¢ *Stakeholder Alignment*: First Discovery session with stakeholder A done. \\n\\n\" +\n",
        "            \"*üöß Challenges*\\n\" +\n",
        "            \"‚Ä¢ Difficulty in aligning stakeholders on the data governance policy, \\n\" +\n",
        "            \"‚Ä¢ Long implementation times for new metrics.\\n\\n\" +\n",
        "            \"*‚û°Ô∏è Next Steps*\\n\" +\n",
        "            \"‚Ä¢ Publish first version of Data governance Policy. All stakeholders should be aligned. \\n\" +\n",
        "            \"‚Ä¢ Start to collect metrics to have SLO and KPIs of the data quality.\\n\" +\n",
        "            \"‚Ä¢ Continue aligning new stakeholders.\"\n",
        "        }}\n",
        "\n",
        "        # Only proceed after saving at teams_json/{start_date}_to_{end_date}/report_summary.json the dictionary generated by you.\n",
        "    \"\"\",\n",
        "    agent=project_analyst,\n",
        "    tools=[JsonFileOperations()],\n",
        "    context = [fups_consolidation_task]\n",
        ")"
      ],
      "metadata": {
        "id": "mtCcjXr0-uX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üî® Creating and running crew"
      ],
      "metadata": {
        "id": "imXrrEZL--Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating crew\n",
        "jira_crew = Crew(\n",
        "    agents=[project_analyst], #, project_analyst],\n",
        "    tasks=fup_tasks + [fups_consolidation_task] + [report_summary_task],\n",
        "    verbose=True,\n",
        "    planning=False,\n",
        "    process=Process.sequential,\n",
        "    embedder={\n",
        "        \"provider\": \"google\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "            \"api_key\": userdata.get('GEMINI_API_KEY'),\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ajLgD9G_-9Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé Human in the loop\n",
        "\n",
        "**Look into the Json files and follow ups**\n",
        "\n",
        "The agents lack context on the entire project, leading to occasional inaccuracies. Manual adjustments might be necessary in the follow-ups and report summary."
      ],
      "metadata": {
        "id": "lkZK8x8qGLuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üö£‚Äç‚ôÇÔ∏è Crew: Slack message sending"
      ],
      "metadata": {
        "id": "XHFnqRFzGi8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üë®‚Äçüíª Communication specialist agent\n",
        "Tasks: Report consolidation & Slack message sending"
      ],
      "metadata": {
        "id": "54pt2TxFGmEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "communication_agent = Agent(\n",
        "    role=\"Communication Integration Specialist\",\n",
        "    goal=\"\"\"\n",
        "    Ensure effective and professional communication of project updates through Slack, managing the consolidation\n",
        "    and delivery of reports while maintaining consistent formatting and clarity in messaging.\n",
        "    \"\"\",\n",
        "    backstory=\"\"\"\n",
        "    You are an experienced Communication Integration Specialist with extensive expertise in corporate communications\n",
        "    and technical report management.\n",
        "\n",
        "    Technical Expertise:\n",
        "    - Expert in communication systems integration\n",
        "    - Proficient in JSON data handling and report consolidation\n",
        "    - Advanced knowledge of Slack API and messaging protocols\n",
        "    - Skilled in automated reporting systems\n",
        "\n",
        "    Communication Skills:\n",
        "    - Excellence in message formatting and presentation\n",
        "    - Strong attention to detail in report consolidation\n",
        "    - Expert in professional communication standards\n",
        "    - Skilled at maintaining consistent messaging tone\n",
        "\n",
        "    Professional Background:\n",
        "    - 10+ years experience in technical communication\n",
        "    - Proven track record in automated reporting systems\n",
        "    - Specialist in data consolidation and presentation\n",
        "    - Expert in corporate communication protocols\n",
        "\n",
        "    Best Practices:\n",
        "    - Ensures data accuracy in consolidated reports\n",
        "    - Maintains professional formatting standards\n",
        "    - Follows systematic verification procedures\n",
        "    - Implements proper error handling\n",
        "    - Adheres to communication security protocols\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=llm,\n",
        "    allow_code_execution=False,\n",
        "    embedder={\n",
        "        \"provider\": \"google\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "            \"api_key\": userdata.get('GEMINI_API_KEY'),\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "0zTrOsF1GrSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üìù Report consolidation task"
      ],
      "metadata": {
        "id": "EwlqCKRqGvvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_consolidation_task = Task(\n",
        "    description=\"\"\"\n",
        "    ## Task Overview\n",
        "    Consolidate the teams report and summary report into a final comprehensive report for the period {start_date} to {end_date}.\n",
        "\n",
        "    ## Objective\n",
        "    Create a unified report that combines the teams report with the summary report while maintaining proper structure and data integrity.\n",
        "\n",
        "    ## Required Action\n",
        "    Use the consolidate_report action from the SlackMessage tool with these parameters:\n",
        "    ```\n",
        "    consolidate_report(\n",
        "        report_data={{\n",
        "            \"teams_report_path\": \"teams_json/{start_date}_to_{end_date}/{start_date}_to_{end_date}.json\",\n",
        "            \"summary_report_path\": \"teams_json/{start_date}_to_{end_date}/report_summary.json\",\n",
        "            \"output_path\": \"teams_json/{start_date}_to_{end_date}/slack_message.json\"\n",
        "        }}\n",
        "    )\n",
        "    ```\n",
        "\n",
        "    ## Success Criteria\n",
        "    1. Teams report and summary report successfully merged\n",
        "    2. All placeholders properly replaced with summary data\n",
        "    3. Consolidated file maintains correct structure\n",
        "    4. Output file properly generated in specified location\n",
        "\n",
        "    ## Validation Steps\n",
        "    - Verify input files exist\n",
        "    - Confirm successful consolidation\n",
        "    - Check data integrity\n",
        "    - Validate JSON format\n",
        "\n",
        "    # These keywords must never be translated and transformed:\n",
        "        - Action:\n",
        "        - Thought:\n",
        "        - Action Input:\n",
        "        because they are part of the thinking process instead of the output.\n",
        "    \"\"\",\n",
        "    expected_output=\"\"\"\n",
        "    Path to the consolidated report file.\n",
        "    \"\"\",\n",
        "    agent=communication_agent,\n",
        "    tools=[SlackMessage()]\n",
        ")"
      ],
      "metadata": {
        "id": "ETNiGcoIHErT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Slack message task"
      ],
      "metadata": {
        "id": "H5kqA2C1HLx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slack_message_task = Task(\n",
        "    description=\"\"\"\n",
        "    ## Task Overview\n",
        "    Send the consolidated report to the specified Slack channel with proper formatting and structure.\n",
        "\n",
        "    ## Objective\n",
        "    Deliver the consolidated report to Slack, ensuring proper message formatting and thread organization.\n",
        "\n",
        "    ## Required Action\n",
        "    Use the send_message action from the SlackMessage tool with these parameters:\n",
        "    ```\n",
        "    send_message(\n",
        "        channel_id = {channel_id},\n",
        "        report_file_path = teams_json/{start_date}_to_{end_date}/slack_message.json\n",
        "    )\n",
        "    ```\n",
        "\n",
        "    ## Success Criteria\n",
        "    1. Message successfully posted\n",
        "    2. All sections properly formatted\n",
        "    3. Team updates correctly threaded\n",
        "    4. All links and emojis properly rendered\n",
        "    5. Proper error handling if message fails\n",
        "\n",
        "    ## Validation Steps\n",
        "    - Verify report file exists\n",
        "    - Confirm message delivery\n",
        "    - Check message formatting\n",
        "    - Validate thread structure\n",
        "\n",
        "    # These keywords must never be translated and transformed:\n",
        "        - Action:\n",
        "        - Thought:\n",
        "        - Action Input:\n",
        "        because they are part of the thinking process instead of the output.\n",
        "    \"\"\",\n",
        "    expected_output=\"\"\"\n",
        "    Dictionary containing the Slack API responses for the main message and threaded replies.\n",
        "    \"\"\",\n",
        "    agent=communication_agent,\n",
        "    tools=[SlackMessage()],\n",
        "    context = [report_consolidation_task]\n",
        ")"
      ],
      "metadata": {
        "id": "ozGnsVAWHM_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üî® Creating and running crew"
      ],
      "metadata": {
        "id": "8i88nVLyHTF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating crew\n",
        "jira_crew = Crew(\n",
        "    agents=[communication_agent],\n",
        "    tasks=[report_consolidation_task, slack_message_task],\n",
        "    verbose=True,\n",
        "    planning=False,\n",
        "    process=Process.sequential,\n",
        "    embedder={\n",
        "        \"provider\": \"google\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "            \"api_key\": userdata.get('GEMINI_API_KEY'),\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "inputs = {\n",
        "    'project_id': \"14130\",\n",
        "    'labels': [\"data-platform-refactoring\"],\n",
        "    'start_date': start_date,\n",
        "    'end_date': end_date,\n",
        "    'json_path': json_path,\n",
        "    'channel_id': channel_id\n",
        "}"
      ],
      "metadata": {
        "id": "fQaix-nRHUVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
